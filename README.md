
<p align="center">
  <a href="https://github.com/shenhao-stu/Transformer-In-CV">
  	<img src="img/logo.png" alt="OCR-by-transformer" height=40% width=40%/>
  </a>
</p>

<p align="center">Python | shenhao0223@163.sufe.edu.cn | ä¸Šæµ·è´¢ç»å¤§å­¦ </p>

<h2 align="center">ğŸ³ğŸ³OCR by transformerğŸ³ğŸ³</h2>

- **Learner : shenhao**
- **Date : 2021.10.19**

> å®ç°å­¦å·æ‰‹å†™æ•°å­—è¯†åˆ«çš„ä»£ç ï¼š

æœ¬æ–‡å°†ä»¥ `ICDAR2015 Incidental Scene Text` ä¸­çš„ [Task 4.3: Word Recognition](https://rrc.cvc.uab.es/?ch=4&com=downloads) å•è¯è¯†åˆ«å­ä»»åŠ¡ä½œä¸ºæ•°æ®é›†ï¼Œè®²è§£å¦‚ä½•ä½¿ç”¨transformeræ¥å®ç°ä¸€ä¸ªç®€å•çš„OCRæ–‡å­—è¯†åˆ«ä»»åŠ¡ï¼Œå¹¶ä»ä¸­ä½“ä¼štransformeræ˜¯å¦‚ä½•åº”ç”¨åˆ°é™¤åˆ†ç±»ä»¥å¤–æ›´å¤æ‚çš„CVä»»åŠ¡ä¸­çš„ã€‚

**æ–‡ç« å°†å¤§è‡´ä»ä»¥ä¸‹å‡ ä¸ªæ–¹é¢è®²è§£ï¼š**

* æ•°æ®é›†ç®€ä»‹
* æ•°æ®åˆ†æä¸å­—ç¬¦æ˜ å°„å…³ç³»æ„å»º
* å¦‚ä½•å°†transformerå¼•å…¥OCR
* è®­ç»ƒæ¡†æ¶ä»£ç è®²è§£

**æœ¬å®éªŒä»£ç ä¸»è¦åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªæ–‡ä»¶ï¼š**

- analysis_recognition_dataset.py (æ•°æ®é›†åˆ†æè„šæœ¬)
- ocr_by_transformer.py (OCRä»»åŠ¡è®­ç»ƒè„šæœ¬)
- transformer.py (transformeræ¨¡å‹æ–‡ä»¶)
- train_utils.py (è®­ç»ƒç›¸å…³è¾…åŠ©å‡½æ•°ï¼Œlossã€optimizerç­‰)
- ocr_by_transformer_colab.ipynb(OCRä»»åŠ¡çš„notebookç¬”è®°)
- ocr_by_transformer_colab.html(HTMLç½‘é¡µ)

å…¶ä¸­ **ocr_by_transformer.py** ä¸º**ä¸»è¦çš„è®­ç»ƒè„šæœ¬**ï¼Œå…¶ä¾æ‰˜ **train_utils.py** å’Œ **transformer.py** ä¸¤ä¸ªæ–‡ä»¶æ„å»º transformer æ¥å®Œæˆ**å­—ç¬¦è¯†åˆ«æ¨¡å‹**çš„è®­ç»ƒã€‚

## æ•°æ®é›†ä»‹ç»

æœ¬æ–‡OCRå®éªŒä½¿ç”¨çš„æ•°æ®é›†åŸºäº`ICDAR2015 Incidental Scene Text` ä¸­çš„ `Task 4.3: Word Recognition`ï¼Œè¿™æ˜¯ä¸€ä¸ªå•è¯è¯†åˆ«ä»»åŠ¡ï¼Œæˆ‘ä»¬å»æ‰äº†å…¶ä¸­ä¸€äº›å›¾ç‰‡ï¼Œæ¥ç®€åŒ–è¿™ä¸ªå®éªŒçš„éš¾åº¦ï¼Œé©¬ä¸Šä¼šæåˆ°ã€‚

å¤„ç†åçš„æ•°æ®é›†ä¸‹è½½é“¾æ¥ï¼š[ç™¾åº¦äº‘ç›˜](https://pan.baidu.com/s/1phOi4Rt023H4cpQS3_IIUQ) ï¼Œå¯†ç : a5rr

è¯¥æ•°æ®é›†åŒ…å«äº†ä¼—å¤šè‡ªç„¶åœºæ™¯å›¾åƒä¸­å‡ºç°çš„æ–‡å­—åŒºåŸŸï¼ŒåŸå§‹æ•°æ®ä¸­è®­ç»ƒé›†å«æœ‰4468å¼ å›¾åƒï¼Œæµ‹è¯•é›†å«æœ‰2077å¼ å›¾åƒï¼Œä»–ä»¬éƒ½æ˜¯ä»åŸå§‹å¤§å›¾ä¸­ä¾æ®æ–‡å­—åŒºåŸŸçš„bounding boxè£å‰ªå‡ºæ¥çš„ï¼Œå›¾åƒä¸­çš„æ–‡å­—åŸºæœ¬å¤„äºå›¾ç‰‡ä¸­å¿ƒä½ç½®ã€‚

|                     word_79.png, "Share"                     |                   word_104.png, "Optical"                    |
| :----------------------------------------------------------: | :----------------------------------------------------------: |
| ![data_share](https://gitee.com/shenhao-stu/picgo/raw/master/Others/data_share.png) | ![data_optical](https://gitee.com/shenhao-stu/picgo/raw/master/Others/data_optical.png) |

ä¸‹è½½åçš„æ•°æ®é›†åŒ…å«ä»¥ä¸‹å‡ ä¸ªæ–‡ä»¶æˆ–ç›®å½•ï¼š

- train  
- train_gt.txt  
- valid  
- valid_gt.txt

å…¶ä¸­ train å’Œ valid ä¸¤ä¸ªç›®å½•åˆ†åˆ«å­˜æ”¾è®­ç»ƒå›¾åƒå’Œæµ‹è¯•å›¾åƒï¼Œå¯¹åº”çš„æ ‡ç­¾æ–‡ä»¶åˆ†åˆ«ä¸º train.txt å’Œ valid.txtï¼Œå½¢å¼ä¸º `å›¾ç‰‡å, "æ–‡å­—æ ‡ç­¾"`ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š

```
word_1.png, "JOINT"
word_2.png, "yourself"
word_3.png, "154"
word_4.png, "197"
word_5.png, "727"
```

ä¸ºäº†ç®€åŒ–åç»­å®éªŒçš„è¯†åˆ«éš¾åº¦ï¼Œæä¾›çš„æ•°æ®é›†ä½¿ç”¨é«˜å®½æ¯”>1.5ç²—ç•¥è¿‡æ»¤äº†æ–‡å­—ç«–å‘æ’åˆ—çš„å›¾åƒï¼Œå› æ­¤ä¸ICDAR2015çš„åŸå§‹æ•°æ®é›†ç•¥æœ‰å·®åˆ«ã€‚

## æ•°æ®åˆ†æä¸å­—ç¬¦æ˜ å°„å…³ç³»æ„å»º

å¼€å§‹å®éªŒå‰ï¼Œæˆ‘ä»¬å…ˆå¯¹æ•°æ®è¿›è¡Œç®€å•åˆ†æï¼Œåªæœ‰å¯¹æ•°æ®çš„ç‰¹æ€§è¶³å¤Ÿäº†è§£ï¼Œæ‰èƒ½å¤Ÿæ›´å¥½çš„æ­å»ºå‡ºbaselineï¼Œåœ¨è®­ç»ƒä¸­å°‘èµ°å¼¯è·¯ã€‚

è¿è¡Œä¸‹é¢ä»£ç ï¼Œå³å¯ä¸€é”®å®Œæˆå¯¹äºæ•°æ®é›†çš„ç®€å•åˆ†æï¼š

> python analysis_recognition_dataset.py

å…·ä½“åœ°ï¼Œè¿™ä¸ªè„šæœ¬æ‰€åšçš„å·¥ä½œåŒ…æ‹¬ï¼šå¯¹æ•°æ®è¿›è¡Œæ ‡ç­¾å­—ç¬¦ç»Ÿè®¡(æœ‰å“ªäº›å­—ç¬¦ã€æ¯ä¸ªå­—ç¬¦å‡ºç°æ¬¡æ•°å¤šå°‘)ã€æœ€é•¿æ ‡ç­¾é•¿åº¦ç»Ÿè®¡ï¼Œå›¾åƒå°ºå¯¸åˆ†æç­‰ï¼Œå¹¶ä¸”æ„å»ºå­—ç¬¦æ ‡ç­¾çš„æ˜ å°„å…³ç³»æ–‡ä»¶ `lbl2id_map.txt`ã€‚

ä¸‹é¢æˆ‘ä»¬æ¥ä¸€ç‚¹ç‚¹çœ‹ä»£ç ï¼š

é¦–å…ˆå®Œæˆå‡†å¤‡å·¥ä½œï¼Œå¯¼å…¥éœ€è¦çš„åº“ï¼Œå¹¶è®¾ç½®å¥½ç›¸å…³ç›®å½•æˆ–æ–‡ä»¶çš„è·¯å¾„

```python
import os
import cv2

# æ•°æ®é›†æ ¹ç›®å½•ï¼Œè¯·å°†æ•°æ®ä¸‹è½½åˆ°æ­¤ä½ç½®
base_data_dir = './ICDAR_2015'

# è®­ç»ƒæ•°æ®é›†å’ŒéªŒè¯æ•°æ®é›†æ‰€åœ¨è·¯å¾„
train_img_dir = os.path.join(base_data_dir, 'train')
valid_img_dir = os.path.join(base_data_dir, 'valid')
# è®­ç»ƒé›†å’ŒéªŒè¯é›†æ ‡ç­¾æ–‡ä»¶è·¯å¾„
train_lbl_path = os.path.join(base_data_dir, 'train_gt.txt')
valid_lbl_path = os.path.join(base_data_dir, 'valid_gt.txt')
# ä¸­é—´æ–‡ä»¶å­˜å‚¨è·¯å¾„ï¼Œå­˜å‚¨æ ‡ç­¾å­—ç¬¦ä¸å…¶idçš„æ˜ å°„å…³ç³»
lbl2id_map_path = os.path.join(base_data_dir, 'lbl2id_map.txt')
```

### 1. æ ‡ç­¾æœ€é•¿å­—ç¬¦ä¸ªæ•°ç»Ÿè®¡

é¦–å…ˆç»Ÿè®¡æ•°æ®é›†æœ€é•¿labelä¸­åŒ…å«çš„å­—ç¬¦æ•°é‡ï¼Œæ­¤å¤„è¦å°†è®­ç»ƒé›†å’ŒéªŒè¯é›†ä¸­çš„æœ€é•¿æ ‡ç­¾éƒ½è¿›è¡Œç»Ÿè®¡ï¼Œè¿›è€Œå¾—åˆ°æœ€é•¿æ ‡ç­¾æ‰€å«å­—ç¬¦ã€‚

```python
def statistics_max_len_label(lbl_path):
    """
    ç»Ÿè®¡æ ‡ç­¾æ–‡ä»¶ä¸­æœ€é•¿çš„labelæ‰€åŒ…å«çš„å­—ç¬¦æ•°
    lbl_path: txtæ ‡ç­¾æ–‡ä»¶è·¯å¾„
    """
    max_len = -1
    with open(lbl_path, 'r', encoding='utf-8') as reader:
        for line in reader:
            items = line.rstrip().split(',')
            # img_name = items[0]  # æå–å›¾åƒåç§°
            lbl_str = items[1].strip()[1:-1]  # æå–æ ‡ç­¾ï¼Œå¹¶é™¤æ‰æ ‡ç­¾ä¸­çš„å¼•å·""
            lbl_len = len(lbl_str)
            max_len = max_len if max_len>lbl_len else lbl_len
    return max_len


train_max_label_len = statistics_max_len_label(train_lbl_path)  # è®­ç»ƒé›†æœ€é•¿label
valid_max_label_len = statistics_max_len_label(valid_lbl_path)  # éªŒè¯é›†æœ€é•¿label
max_label_len = max(train_max_label_len, valid_max_label_len)  # å…¨æ•°æ®é›†æœ€é•¿label
print(f"æ•°æ®é›†ä¸­åŒ…å«å­—ç¬¦æœ€å¤šçš„labelé•¿åº¦ä¸º{max_label_len}")
```

æ•°æ®é›†ä¸­æœ€é•¿labelå«æœ‰21ä¸ªå­—ç¬¦ï¼Œè¿™å°†ä¸ºåé¢transformeræ¨¡å‹æ­å»ºæ—¶çš„æ—¶é—´æ­¥é•¿åº¦çš„è®¾ç½®æä¾›å‚è€ƒã€‚

### 2. æ ‡ç­¾æ‰€å«å­—ç¬¦ç»Ÿè®¡

ä¸‹é¢ä»£ç æŸ¥çœ‹æ•°æ®é›†ä¸­å‡ºç°è¿‡çš„æ‰€æœ‰å­—ç¬¦ï¼š

```python
def statistics_label_cnt(lbl_path, lbl_cnt_map):
    """
    ç»Ÿè®¡æ ‡ç­¾æ–‡ä»¶ä¸­labeléƒ½åŒ…å«å“ªäº›å­—ç¬¦ä»¥åŠå„è‡ªå‡ºç°çš„æ¬¡æ•°
    lbl_path : æ ‡ç­¾æ–‡ä»¶æ‰€å¤„è·¯å¾„
    lbl_cnt_map : è®°å½•æ ‡ç­¾ä¸­å­—ç¬¦å‡ºç°æ¬¡æ•°çš„å­—å…¸
    """

    with open(lbl_path, 'r', encoding='utf-8') as reader:
        for line in reader:
            items = line.rstrip().split(',')
            # img_name = items[0]
            lbl_str = items[1].strip()[1:-1]  # æå–æ ‡ç­¾å¹¶å»é™¤labelä¸­çš„åŒå¼•å·""
            for lbl in lbl_str:
                if lbl not in lbl_cnt_map.keys():
                    lbl_cnt_map[lbl] = 1
                else:
                    lbl_cnt_map[lbl] += 1


lbl_cnt_map = dict()  # ç”¨äºå­˜å‚¨å­—ç¬¦å‡ºç°æ¬¡æ•°çš„å­—å…¸
statistics_label_cnt(train_lbl_path, lbl_cnt_map)  # è®­ç»ƒé›†ä¸­å­—ç¬¦å‡ºç°æ¬¡æ•°ç»Ÿè®¡
print("è®­ç»ƒé›†ä¸­labelä¸­å‡ºç°çš„å­—ç¬¦:")
print(lbl_cnt_map)
statistics_label_cnt(valid_lbl_path, lbl_cnt_map)  # è®­ç»ƒé›†å’ŒéªŒè¯é›†ä¸­å­—ç¬¦å‡ºç°æ¬¡æ•°ç»Ÿè®¡
print("è®­ç»ƒé›†+éªŒè¯é›†labelä¸­å‡ºç°çš„å­—ç¬¦:")
print(lbl_cnt_map)
```

è¾“å‡ºç»“æœä¸ºï¼š 

``` python
è®­ç»ƒé›†ä¸­labelä¸­å‡ºç°çš„å­—ç¬¦:
{'[': 2, '0': 182, '6': 38, ']': 2, '2': 119, '-': 68, '3': 50, 'C': 593, 'a': 843, 'r': 655, 'p': 197, 'k': 96, 'E': 1421, 'X': 110, 'I': 861, 'T': 896, 'R': 836, 'f': 133, 'u': 293, 's': 557, 'i': 651, 'o': 659, 'n': 605, 'l': 408, 'e': 1055, 'v': 123, 'A': 1189, 'U': 319, 'O': 965, 'N': 785, 'c': 318, 't': 563, 'm': 202, 'W': 179, 'H': 391, 'Y': 229, 'P': 389, 'F': 259, 'G': 345, '?': 5, 'S': 1161, 'b': 88, 'h': 299, ' ': 50, 'g': 171, 'L': 745, 'M': 367, 'D': 383, 'd': 257, '$': 46, '5': 77, '4': 44, '.': 95, 'w': 97, 'B': 331, '1': 184, '7': 43, '8': 44, 'V': 158, 'y': 161, 'K': 163, '!': 51, '9': 66, 'z': 12, ';': 3, '#': 16, 'j': 15, "'": 51, 'J': 72, ':': 19, 'x': 27, '%': 28, '/': 24, 'q': 3, 'Q': 19, '(': 6, ')': 5, '\\': 8, '"': 8, 'Ã‚': 3, 'Â´': 3, 'Z': 29, '&': 9, 'Ãƒ': 1, 'â€°': 1, '@': 4, '=': 1, '+': 1}
è®­ç»ƒé›†+éªŒè¯é›†labelä¸­å‡ºç°çš„å­—ç¬¦:
{'[': 2, '0': 232, '6': 44, ']': 2, '2': 139, '-': 87, '3': 69, 'C': 893, 'a': 1200, 'r': 935, 'p': 317, 'k': 137, 'E': 2213, 'X': 181, 'I': 1241, 'T': 1315, 'R': 1262, 'f': 203, 'u': 415, 's': 793, 'i': 924, 'o': 954, 'n': 880, 'l': 555, 'e': 1534, 'v': 169, 'A': 1827, 'U': 467, 'O': 1440, 'N': 1158, 'c': 442, 't': 829, 'm': 278, 'W': 288, 'H': 593, 'Y': 341, 'P': 582, 'F': 402, 'G': 521, '?': 7, 'S': 1748, 'b': 129, 'h': 417, ' ': 82, 'g': 260, 'L': 1120, 'M': 536, 'D': 548, 'd': 367, '$': 57, '5': 100, '4': 53, '.': 132, 'w': 136, 'B': 468, '1': 228, '7': 60, '8': 51, 'V': 224, 'y': 231, 'K': 253, '!': 65, '9': 76, 'z': 14, ';': 3, '#': 24, 'j': 19, "'": 70, 'J': 100, ':': 24, 'x': 38, '%': 42, '/': 29, 'q': 3, 'Q': 28, '(': 7, ')': 5, '\\': 8, '"': 8, 'Ã‚': 3, 'Â´': 3, 'Z': 36, '&': 15, 'Ãƒ': 3, 'â€°': 2, '@': 9, '=': 1, '+': 2, 'Â©': 1}
```

ä¸Šæ–¹ä»£ç ä¸­ï¼Œ**lbl_cnt_map** ä¸ºå­—ç¬¦å‡ºç°æ¬¡æ•°çš„ç»Ÿè®¡å­—å…¸ï¼Œåé¢è¿˜ä¼šç”¨äºå»ºç«‹å­—ç¬¦åŠå…¶idæ˜ å°„å…³ç³»ã€‚

ä»æ•°æ®é›†ç»Ÿè®¡ç»“æœæ¥çœ‹ï¼Œæµ‹è¯•é›†å«æœ‰è®­ç»ƒé›†æ²¡æœ‰å‡ºç°è¿‡çš„å­—ç¬¦ï¼Œä¾‹å¦‚æµ‹è¯•é›†ä¸­åŒ…å«1ä¸ª'Â©'æœªæ›¾åœ¨è®­ç»ƒé›†å‡ºç°ã€‚è¿™ç§æƒ…å†µæ•°é‡ä¸å¤šï¼Œåº”è¯¥é—®é¢˜ä¸å¤§ï¼Œæ‰€ä»¥æ­¤å¤„æœªå¯¹æ•°æ®é›†è¿›è¡Œé¢å¤–å¤„ç†(ä½†æ˜¯æœ‰æ„è¯†çš„è¿›è¡Œè¿™ç§è®­ç»ƒé›†å’Œæµ‹è¯•é›†æ˜¯å¦å­˜åœ¨diffçš„æ£€æŸ¥æ˜¯å¿…è¦çš„)ã€‚

### 3. charå’Œidçš„æ˜ å°„å­—å…¸æ„å»º

åœ¨æœ¬æ–‡OCRä»»åŠ¡ä¸­ï¼Œéœ€è¦å¯¹å›¾ç‰‡ä¸­çš„æ¯ä¸ªå­—ç¬¦è¿›è¡Œé¢„æµ‹ï¼Œä¸ºäº†è¾¾åˆ°è¿™ä¸ªç›®çš„ï¼Œé¦–å…ˆå°±éœ€è¦å»ºç«‹ä¸€ä¸ªå­—ç¬¦ä¸å…¶idçš„æ˜ å°„å…³ç³»ï¼Œå°†æ–‡æœ¬ä¿¡æ¯è½¬åŒ–ä¸ºå¯ä¾›æ¨¡å‹è¯»å–çš„æ•°å­—ä¿¡æ¯ï¼Œè¿™ä¸€æ­¥ç±»ä¼¼NLPä¸­å»ºç«‹è¯­æ–™åº“ã€‚

åœ¨æ„å»ºæ˜ å°„å…³ç³»æ—¶ï¼Œé™¤äº†è®°å½•æ‰€æœ‰æ ‡ç­¾æ–‡ä»¶ä¸­å‡ºç°çš„å­—ç¬¦å¤–ï¼Œè¿˜éœ€è¦åˆå§‹åŒ–ä¸‰ä¸ªç‰¹æ®Šå­—ç¬¦ï¼Œåˆ†åˆ«ç”¨æ¥ä»£è¡¨ä¸€ä¸ªå¥å­èµ·å§‹ç¬¦ã€å¥å­ç»ˆæ­¢ç¬¦å’Œå¡«å……(Padding)æ ‡è¯†ç¬¦ã€‚ç›¸ä¿¡ç»è¿‡6.1èŠ‚çš„ä»‹ç»ä½ èƒ½å¤Ÿæ˜ç™½è¿™3ç§ç‰¹æ®Šå­—ç¬¦çš„ä½œç”¨ï¼Œåé¢datasetæ„å»ºéƒ¨åˆ†çš„è®²è§£ä¹Ÿè¿˜ä¼šå†æ¬¡æåˆ°ã€‚

è„šæœ¬è¿è¡Œåï¼Œæ‰€æœ‰å­—ç¬¦çš„æ˜ å°„å…³ç³»å°†ä¼šä¿å­˜åœ¨ `lbl2id_map.txt`æ–‡ä»¶ä¸­ã€‚

```python
# æ„é€ labelä¸­ å­—ç¬¦--id ä¹‹é—´çš„æ˜ å°„
print("æ„é€ labelä¸­ å­—ç¬¦--id ä¹‹é—´çš„æ˜ å°„:")

lbl2id_map = dict()
# åˆå§‹åŒ–ä¸‰ä¸ªç‰¹æ®Šå­—ç¬¦
lbl2id_map['â˜¯'] = 0    # paddingæ ‡è¯†ç¬¦
lbl2id_map['â– '] = 1    # å¥å­èµ·å§‹ç¬¦
lbl2id_map['â–¡'] = 2    # å¥å­ç»“æŸç¬¦
# ç”Ÿæˆå…¶ä½™å­—ç¬¦çš„idæ˜ å°„å…³ç³»
cur_id = 3
for lbl in lbl_cnt_map.keys():
    lbl2id_map[lbl] = cur_id
    cur_id += 1
    
# ä¿å­˜ å­—ç¬¦--id ä¹‹é—´çš„æ˜ å°„ åˆ°txtæ–‡ä»¶
with open(lbl2id_map_path, 'w', encoding='utf-8') as writer:  # å‚æ•°encodingæ˜¯å¯é€‰é¡¹ï¼Œéƒ¨åˆ†è®¾å¤‡å¹¶æœªé»˜è®¤ä¸ºutf-8
    for lbl in lbl2id_map.keys():
        cur_id = lbl2id_map[lbl]
        print(lbl, cur_id)
        line = lbl + '\t' + str(cur_id) + '\n'
        writer.write(line)
```

æ„é€ å‡ºçš„ å­—ç¬¦-id ä¹‹é—´çš„æ˜ å°„:

```
â˜¯ 0
â–  1
â–¡ 2
[ 3
0 4
...
Z 83
& 84
Ã‰ 85
@ 86
= 87
+ 88
Ã© 89
```

æ­¤å¤–ï¼Œ**analysis_recognition_dataset.py** æ–‡ä»¶ä¸­è¿˜åŒ…å«ä¸€ä¸ªå»ºç«‹å…³ç³»æ˜ å°„å­—å…¸çš„å‡½æ•°ï¼Œå¯ä»¥é€šè¿‡è¯»å–å«æœ‰æ˜ å°„å…³ç³»txtçš„æ–‡ä»¶ï¼Œæ„å»ºå‡ºå­—ç¬¦åˆ°idå’Œidåˆ°å­—ç¬¦çš„æ˜ å°„å­—å…¸ã€‚è¿™æœåŠ¡äºåç»­transformerè®­ç»ƒè¿‡ç¨‹ï¼Œä»¥æ–¹ä¾¿å­—ç¬¦å…³ç³»å¿«é€Ÿå®ç°è½¬æ¢ã€‚

```python
def load_lbl2id_map(lbl2id_map_path):
    """
    è¯»å– å­—ç¬¦-id æ˜ å°„å…³ç³»è®°å½•çš„txtæ–‡ä»¶ï¼Œå¹¶è¿”å› lbl->id å’Œ id->lbl æ˜ å°„å­—å…¸
    lbl2id_map_path : å­—ç¬¦-id æ˜ å°„å…³ç³»è®°å½•çš„txtæ–‡ä»¶è·¯å¾„
    """

    lbl2id_map = dict()
    id2lbl_map = dict()
    with open(lbl2id_map_path, 'r', encoding='utf-8') as reader:
        for line in reader:
            items = line.rstrip().split('\t')
            label = items[0]
            cur_id = int(items[1])
            lbl2id_map[label] = cur_id
            id2lbl_map[cur_id] = label
    return lbl2id_map, id2lbl_map
```

### 4. æ•°æ®é›†å›¾åƒå°ºå¯¸åˆ†æ

åœ¨è¿›è¡Œå›¾åƒåˆ†ç±»æ£€æµ‹ç­‰ä»»åŠ¡æ—¶ï¼Œç»å¸¸ä¼šæŸ¥çœ‹å›¾åƒçš„å°ºå¯¸åˆ†å¸ƒï¼Œè¿›è€Œç¡®å®šåˆé€‚çš„å›¾åƒçš„é¢„å¤„ç†æ–¹å¼ï¼Œä¾‹å¦‚åœ¨è¿›è¡Œç›®æ ‡æ£€æµ‹æ—¶ä¼šå¯¹å›¾åƒå°ºå¯¸å’Œbounding boxçš„å°ºå¯¸è¿›è¡Œç»Ÿè®¡ï¼Œåˆ†æé•¿å®½æ¯”è¿›è€Œé€‰æ‹©åˆé€‚çš„å›¾åƒè£å‰ªç­–ç•¥å’Œé€‚å½“çš„åˆå§‹anchorç­–ç•¥ç­‰ã€‚

å› æ­¤è¿™é‡Œé€šè¿‡åˆ†æå›¾åƒå®½åº¦ã€é«˜åº¦å’Œå®½é«˜æ¯”ç­‰ä¿¡æ¯æ¥äº†è§£æ•°æ®çš„ç‰¹ç‚¹ï¼Œä¸ºåç»­å®éªŒç­–ç•¥åˆ¶å®šæä¾›å‚è€ƒã€‚

```python
# åˆ†ææ•°æ®é›†å›¾ç‰‡å°ºå¯¸
print("åˆ†ææ•°æ®é›†å›¾ç‰‡å°ºå¯¸:")

# åˆå§‹åŒ–å‚æ•°
min_h = 1e10
min_w = 1e10
max_h = -1
max_w = -1
min_ratio = 1e10
max_ratio = 0
# éå†æ•°æ®é›†è®¡ç®—å°ºå¯¸ä¿¡æ¯
for img_name in os.listdir(train_img_dir):
    img_path = os.path.join(train_img_dir, img_name)
    img = cv2.imread(img_path)  # è¯»å–å›¾ç‰‡
    h, w = img.shape[:2]  # æå–å›¾åƒé«˜å®½ä¿¡æ¯
    ratio = w / h  # å®½é«˜æ¯”
    min_h = min_h if min_h <= h else h  # æœ€å°å›¾ç‰‡é«˜åº¦
    max_h = max_h if max_h >= h else h  # æœ€å¤§å›¾ç‰‡é«˜åº¦
    min_w = min_w if min_w <= w else w  # æœ€å°å›¾ç‰‡å®½åº¦
    max_w = max_w if max_w >= w else w  # æœ€å¤§å›¾ç‰‡å®½åº¦
    min_ratio = min_ratio if min_ratio <= ratio else ratio  # æœ€å°å®½é«˜æ¯”
    max_ratio = max_ratio if max_ratio >= ratio else ratio  # æœ€å¤§å®½é«˜æ¯”
# è¾“å‡ºä¿¡æ¯
print('min_h:', min_h)
print('max_h:', max_h)
print('min_w:', min_w)
print('max_w:', max_w)
print('min_ratio:', min_ratio)
print('max_ratio:', max_ratio)
```

æ•°æ®é›†å›¾ç‰‡å°ºå¯¸ç›¸å…³æƒ…å†µç»Ÿè®¡ç»“æœå¦‚ä¸‹ï¼š

```
min_h: 9
max_h: 295
min_w: 16
max_w: 628
min_ratio: 0.6666666666666666
max_ratio: 8.619047619047619
```

é€šè¿‡ä»¥ä¸Šçš„ç»“æœï¼Œå¯çœ‹å‡ºå›¾ç‰‡å¤šä¸ºå§å€’çš„é•¿æ¡å½¢ï¼Œæœ€å¤§å®½é«˜æ¯” > 8 å¯è§å­˜åœ¨æç»†é•¿çš„å›¾ç‰‡ã€‚

ä»¥ä¸Šä¾¿æ˜¯å¯¹äºæ•°æ®é›†çš„è‹¥å¹²ç®€å•åˆ†æï¼Œå¹¶ä¸”å‡†å¤‡å‡ºäº†è®­ç»ƒè¦ç”¨çš„char2idæ˜ å°„æ–‡ä»¶ï¼Œä¸‹é¢å°±æ˜¯é‡å¤´æˆäº†ï¼Œæ¥çœ‹çœ‹æˆ‘ä»¬å¦‚ä½•å°†transfomerå¼•å…¥ï¼Œæ¥å®ŒæˆOCRå•è¯è¯†åˆ«è¿™æ ·çš„CVä»»åŠ¡ã€‚

## å¦‚ä½•å°†transformerå¼•å…¥OCR

å¾ˆå¤šç®—æ³•æœ¬èº«å¹¶ä¸éš¾ï¼Œéš¾çš„æ˜¯å¦‚ä½•æ€è€ƒå’Œå®šä¹‰é—®é¢˜ï¼ŒæŠŠå®ƒè½¬åŒ–åˆ°å·²çŸ¥çš„è§£å†³æ–¹æ¡ˆä¸Šå»ã€‚å› æ­¤åœ¨çœ‹ä»£ç ä¹‹å‰ï¼Œæˆ‘ä»¬å…ˆè¦èŠèŠï¼Œä¸ºä»€ä¹ˆtransformerå¯ä»¥è§£å†³OCRé—®é¢˜ï¼ŒåŠ¨æœºæ˜¯ä»€ä¹ˆï¼Ÿ

é¦–å…ˆï¼Œæˆ‘ä»¬çŸ¥é“ï¼Œtransformerè¢«å¹¿æ³›åº”ç”¨åœ¨NLPé¢†åŸŸä¸­ï¼Œå¯ä»¥è§£å†³ç±»ä¼¼æœºå™¨ç¿»è¯‘è¿™æ ·çš„sequence to sequenceç±»çš„é—®é¢˜ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºï¼š

<img src="img/Seq2Seq.png" alt="Seq2Seqæ¨¡å‹" style="zoom: 33%;" />

è€ŒOCRè¯†åˆ«ä»»åŠ¡ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œæˆ‘ä»¬å¸Œæœ›å°†ä¸‹å›¾è¯†åˆ«ä¸º"Share"ï¼Œæœ¬è´¨ä¸Šä¹Ÿå¯ä»¥çœ‹ä½œæ˜¯ä¸€ä¸ªsequence to sequenceä»»åŠ¡ï¼Œåªä¸è¿‡è¾“å…¥çš„åºåˆ—ä¿¡æ¯æ˜¯ç”±å›¾ç‰‡å½¢å¼è¡¨ç¤ºçš„ã€‚å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š

<img src="img/Seq2Seq in OCR.png" alt="Seq2Seq in OCRæ¨¡å‹" style="zoom:50%;" />

é€šè¿‡è§‚å¯Ÿä¸Šå›¾å¯ä»¥å‘ç°ï¼Œæ•´ä¸ªpipelineå’Œåˆ©ç”¨transformerè®­ç»ƒæœºå™¨ç¿»è¯‘çš„æµç¨‹æ˜¯åŸºæœ¬ä¸€è‡´çš„ï¼Œä¹‹é—´çš„å·®å¼‚ä¸»è¦æ˜¯å¤šäº†**å€ŸåŠ©ä¸€ä¸ªCNNç½‘ç»œä½œä¸ºbackbone**æå–å›¾åƒç‰¹å¾å¾—åˆ°**input embedding**çš„è¿‡ç¨‹ã€‚

å…³äºæ„é€ transformerçš„è¾“å…¥embeddingè¿™éƒ¨åˆ†çš„è®¾è®¡ï¼Œæ˜¯æœ¬æ–‡çš„é‡ç‚¹ï¼Œä¹Ÿæ˜¯æ•´ä¸ªç®—æ³•èƒ½å¤Ÿworkçš„å…³é”®ã€‚åæ–‡ä¼šç»“åˆä»£ç ï¼Œå¯¹ä¸Šé¢ç¤ºæ„å›¾ä¸­å±•ç¤ºçš„ç›¸å…³ç»†èŠ‚è¿›è¡Œå±•å¼€è®²è§£

## å‡†å¤‡å·¥ä½œ

### å¯¼å…¥åº“


```python
import os
import time
import copy
import numpy as np
from PIL import Image
import matplotlib.pyplot as plt

# torchç›¸å…³åŒ…
import torch
import torch.nn as nn
import torchvision.models as models
import torchvision.transforms as transforms

# å¯¼å…¥å·¥å…·ç±»åŒ…
from analysis_recognition_dataset import load_lbl2id_map, statistics_max_len_label
from transformer import *
from train_utils import *
```

### æ•°æ®é›†æ„é€ å’Œå‚æ•°è®¾å®š

#### set parameters


```python
# TODO set parameters
# æ•°æ®é›†æ ¹ç›®å½•ï¼Œè¯·å°†æ•°æ®ä¸‹è½½åˆ°æ­¤ä½ç½®
base_data_dir = '../dataset/ICDAR_2015/'
device_name = 'cuda' if torch.cuda.is_available() else 'cpu'
device = torch.device(device_name)
nrof_epochs = 20
batch_size = 64
model_save_path = './log/ex1_ocr_model.pth'
print('Using {} device'.format(device))
```

    Using cuda device


#### The preparatory work for  datasets


```python
# è¯»å–label-idæ˜ å°„å…³ç³»è®°å½•æ–‡ä»¶
lbl2id_map_path = os.path.join(base_data_dir, 'lbl2id_map.txt')
lbl2id_map, id2lbl_map = load_lbl2id_map(lbl2id_map_path)

# ç»Ÿè®¡æ•°æ®é›†ä¸­å‡ºç°çš„æ‰€æœ‰çš„labelä¸­åŒ…å«å­—ç¬¦æœ€å¤šçš„æœ‰å¤šå°‘å­—ç¬¦ï¼Œæ•°æ®é›†æ„é€ gtä¿¡æ¯éœ€è¦ç”¨åˆ°
train_lbl_path = os.path.join(base_data_dir, 'train_gt.txt')
valid_lbl_path = os.path.join(base_data_dir, 'valid_gt.txt')
train_max_label_len = statistics_max_len_label(train_lbl_path)  # train_max_label_len = 19
valid_max_label_len = statistics_max_len_label(valid_lbl_path)  # valid_max_label_len = 21

# æ•°æ®é›†ä¸­å­—ç¬¦æ•°æœ€å¤šçš„ä¸€ä¸ªcaseä½œä¸ºåˆ¶ä½œçš„gtçš„sequence_len
sequence_len = max(train_max_label_len, valid_max_label_len) # sequence_len = 21
```

## Datasetæ„å»º

### å›¾ç‰‡é¢„å¤„ç†

å‡è®¾å›¾ç‰‡å°ºå¯¸ä¸º $ [batch\_size, 3, H_i, W_i] $

ç»è¿‡ç½‘ç»œåçš„ç‰¹å¾å›¾å°ºå¯¸ä¸º $ [batch\_size, C_f, H_f ,W_f] $

åŸºäºä¹‹å‰å¯¹äºæ•°æ®é›†çš„åˆ†æï¼Œå›¾ç‰‡åŸºæœ¬éƒ½æ˜¯æ°´å¹³é•¿æ¡çŠ¶çš„ï¼Œå›¾åƒå†…å®¹æ˜¯æ°´å¹³æ’åˆ—çš„å­—ç¬¦ç»„æˆçš„å•è¯ã€‚é‚£ä¹ˆå›¾ç‰‡ç©ºé—´ä¸ŠåŒä¸€çºµå‘åˆ‡ç‰‡çš„ä½ç½®ï¼ŒåŸºæœ¬åªæœ‰ä¸€ä¸ªå­—ç¬¦ï¼Œå› æ­¤çºµå‘åˆ†è¾¨ç‡ä¸éœ€è¦å¾ˆå¤§ï¼Œé‚£ä¹ˆå– $H_f=1$å³å¯ï¼›è€Œæ¨ªå‘çš„åˆ†è¾¨ç‡éœ€è¦å¤§ä¸€äº›ï¼Œæˆ‘ä»¬éœ€è¦æœ‰ä¸åŒçš„embeddingæ¥ç¼–ç æ°´å¹³æ–¹å‘ä¸Šä¸åŒå­—ç¬¦çš„ç‰¹å¾ã€‚

![WH](img/WH.png)

è¿™é‡Œï¼Œæˆ‘ä»¬å°±ç”¨æœ€ç»å…¸çš„**resnet18**ç½‘ç»œä½œä¸ºbackboneï¼Œç”±äºå…¶**ä¸‹é‡‡æ ·å€æ•°**ä¸º**32**ï¼Œæœ€åä¸€å±‚ç‰¹å¾å›¾**channelæ•°**ä¸º512ï¼Œé‚£ä¹ˆ:

$ H_i = H_f * 32 = 32 $

$ C_f = 512 $

é‚£ä¹ˆè¾“å…¥å›¾ç‰‡çš„å®½åº¦å¦‚ä½•ç¡®å®šå‘¢ï¼Ÿè¿™é‡Œç»™å‡ºä¸¤ç§æ–¹æ¡ˆï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºï¼š

![resizeæ–¹æ¡ˆ](img/resize.png)

**æ–¹æ³•ä¸€ï¼š** è®¾å®šä¸€ä¸ªå›ºå®šå°ºå¯¸ï¼Œå°†å›¾åƒä¿æŒå…¶å®½é«˜æ¯”è¿›è¡Œresizeï¼Œå³ä¾§ç©ºä½™åŒºåŸŸè¿›è¡Œpaddingï¼›

**æ–¹æ³•äºŒï¼š** ç›´æ¥å°†åŸå§‹å›¾åƒå¼ºåˆ¶resizeåˆ°ä¸€ä¸ªé¢„è®¾çš„å›ºå®šå°ºå¯¸ã€‚

> **æ³¨ï¼šè¿™é‡Œä¸å¦¨å…ˆæ€è€ƒä¸‹ï¼Œä½ è§‰å¾—å“ªç§æ–¹æ¡ˆæ¯”è¾ƒå¥½å‘¢ï¼Ÿ**  
è¿™é‡Œä½œè€…é€‰æ‹©äº†æ–¹æ³•ä¸€ï¼Œå› ä¸ºå›¾ç‰‡çš„å®½é«˜æ¯”å’Œå›¾ç‰‡ä¸­å•è¯çš„å­—ç¬¦æ•°é‡æ˜¯å¤§è‡´å‘ˆæ­£æ¯”çš„ï¼Œå¦‚æœé¢„å¤„ç†æ—¶ä¿æŒä½åŸå›¾ç‰‡çš„å®½é«˜æ¯”ï¼Œé‚£ä¹ˆç‰¹å¾å›¾ä¸Šæ¯ä¸€ä¸ªåƒç´ å¯¹åº”åŸå›¾ä¸Šå­—ç¬¦åŒºåŸŸçš„èŒƒå›´å°±æ˜¯åŸºæœ¬ç¨³å®šçš„ï¼Œè¿™æ ·æˆ–è®¸æœ‰æ›´å¥½çš„é¢„æµ‹æ•ˆæœã€‚

è¿™é‡Œè¿˜æœ‰ä¸ªç»†èŠ‚ï¼Œè§‚å¯Ÿä¸Šå›¾ä½ ä¼šå‘ç°ï¼Œæ¯ä¸ªå®½ï¼šé«˜=1:1çš„åŒºåŸŸå†…ï¼ŒåŸºæœ¬éƒ½åˆ†å¸ƒç€2-3ä¸ªå­—ç¬¦ï¼Œå› æ­¤æˆ‘ä»¬å®é™…æ“ä½œæ—¶ä¹Ÿæ²¡æœ‰ä¸¥æ ¼çš„ä¿æŒå®½é«˜æ¯”ä¸å˜ï¼Œè€Œæ˜¯å°†å®½é«˜æ¯”æå‡äº†3å€ï¼Œå³å…ˆå°†åŸå§‹å›¾ç‰‡å®½åº¦æ‹‰é•¿åˆ°åŸæ¥çš„3å€ï¼Œå†ä¿æŒå®½é«˜æ¯”ï¼Œå°†é«˜resizeåˆ°32ã€‚

> **æ³¨ï¼šè¿™é‡Œå»ºè®®å†æ¬¡åœä¸‹æ¥æ€è€ƒä¸‹ï¼Œåˆšåˆšè¿™ä¸ªç»†èŠ‚åˆæ˜¯ä¸ºä»€ä¹ˆï¼Ÿ**  
è¿™æ ·åšçš„ç›®çš„æ˜¯è®©å›¾ç‰‡ä¸Šæ¯ä¸€ä¸ªå­—ç¬¦ï¼Œéƒ½æœ‰è‡³å°‘ä¸€ä¸ªç‰¹å¾å›¾ä¸Šçš„åƒç´ ä¸ä¹‹å¯¹åº”ï¼Œè€Œä¸æ˜¯ç‰¹å¾å›¾å®½ç»´åº¦ä¸Šä¸€ä¸ªåƒç´ åŒæ—¶ç¼–ç äº†åŸå›¾ä¸­çš„å¤šä¸ªå­—ç¬¦çš„ä¿¡æ¯ï¼Œè¿™æ ·æˆ‘è®¤ä¸ºä¼šå¯¹transformerçš„é¢„æµ‹å¸¦æ¥ä¸å¿…è¦çš„å›°éš¾ã€‚

ç¡®å®šäº†resizeæ–¹æ¡ˆï¼Œ$ W_i $ å…·ä½“è®¾ç½®ä¸ºå¤šå°‘å‘¢ï¼Ÿç»“åˆå‰é¢æˆ‘ä»¬å¯¹æ•°æ®é›†åšåˆ†ææ—¶çš„ä¸¤ä¸ªé‡è¦æŒ‡æ ‡ï¼Œæ•°æ®é›†labelä¸­æœ€é•¿å­—ç¬¦æ•°ä¸º21ï¼Œæœ€é•¿çš„å®½é«˜æ¯”8.6ï¼Œæˆ‘ä»¬å°†æœ€ç»ˆçš„å®½é«˜æ¯”è®¾ç½®ä¸º 24:1ï¼Œå› æ­¤æ±‡æ€»ä¸€ä¸‹å„ä¸ªå‚æ•°çš„è®¾ç½®ï¼š

$ H_i = H_f * 32 = 32 $

$ W_i = 24 * H_i = 768 $

$ C_f = 512, H_f = 1, W_f = 24 $


```python
img_dir = base_data_dir + 'train'
img_path = os.path.join(img_dir, 'word_2.png')
img_path
```


    '../dataset/ICDAR_2015/train/word_2.png'


```python
# load image
img = Image.open(img_path).convert('RGB')
max_ratio =8 
# å¯¹å›¾ç‰‡è¿›è¡Œå¤§è‡´ç­‰æ¯”ä¾‹çš„ç¼©æ”¾
# å°†é«˜ç¼©æ”¾åˆ°32ï¼Œå®½å¤§è‡´ç­‰æ¯”ä¾‹ç¼©æ”¾ï¼Œä½†è¦è¢«32æ•´é™¤
w, h = img.size
ratio = round((w / h) * 3)   # å°†å®½æ‹‰é•¿3å€ï¼Œç„¶åå››èˆäº”å…¥
if ratio == 0:
    ratio = 1 
if ratio > max_ratio:
    ratio = max_ratio
h_new = 32
w_new = h_new * ratio
img_resize = img.resize((w_new, h_new), Image.BILINEAR)

# å¯¹å›¾ç‰‡å³åŠè¾¹è¿›è¡Œpaddingï¼Œä½¿å¾— å®½/é«˜ æ¯”ä¾‹å›ºå®š=self.max_ratio
# Image.new(mode, size, color=0)
img_padd = Image.new('RGB', (32*max_ratio, 32), (0,0,0)) # (0,0,0)ä¸ºRGBä¸‰é€šé“çš„å¡«å……è‰²
img_padd.paste(img_resize, (0, 0))  # åœ¨(0,0)ä½ç½®èµ·è¿›è¡Œpaste
```

é€šè¿‡ä¸‹å›¾å¯ä»¥çœ‹åˆ°,åœ¨å˜åŒ–åçš„IMAGEä¸­æ¯ä¸€ä¸ª32\*32çš„åŒºåŸŸä¸­éƒ½åªåŒ…æ‹¬äº†ä¸€ä¸ªå­—ç¬¦,å› æ­¤ç‰¹å¾å›¾ä¸Šçš„åƒç´ ä¸ä¼šåŒæ—¶ç¼–ç åŸå›¾ä¸­çš„å¤šä¸ªå­—ç¬¦çš„ä¿¡æ¯.åªå¯èƒ½å¯¹åº”äºä¸€ä¸ªå­—ç¬¦.


```python
_, [ax1, ax2] = plt.subplots(1, 2, sharex=True, sharey=True)
ax1.imshow(np.array(img))
ax2.imshow(np.array(img_padd))
plt.show()
```


![png](img/output_22_0.png)
    


### å›¾åƒå¢å¹¿

å›¾åƒå¢å¹¿å¹¶ä¸æ˜¯é‡ç‚¹ï¼Œè¿™é‡Œæˆ‘ä»¬é™¤äº†ä¸Šè¿°çš„resizeæ–¹æ¡ˆå¤–ï¼Œä»…å¯¹å›¾åƒè¿›è¡Œå¸¸è§„çš„éšæœºé¢œè‰²å˜æ¢å’Œå½’ä¸€åŒ–æ“ä½œã€‚


```python
# transforms.ColorJitter(brightness=0, contrast=0, saturation=0, hue=0)
# transforms.ColorJitter?
```


```python
# å®šä¹‰éšæœºé¢œè‰²å˜æ¢
color_trans = transforms.ColorJitter(0.1, 0.1, 0.1) # range: 0.9-1.1 for brightness,contrast and saturation
# å®šä¹‰ Normalize
trans_Normalize = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]), # torchvision.transforms.Normalize(mean, std)
])
# éšæœºé¢œè‰²å˜æ¢
img_input_color = color_trans(img_padd)
# Normalize
img_input_norm = trans_Normalize(img_input_color)
```


```python
_, [ax1, ax2] = plt.subplots(1, 2, sharex=True, sharey=True)
ax1.imshow(np.array(img_input_color))
ax2.imshow(np.transpose(img_input_norm, (1, 2, 0)))
plt.show()
```

    Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).




![png](img/output_27_1.png)
    


### å®Œæ•´ä»£ç 


```python
class Recognition_Dataset(object):

    def __init__(self, dataset_root_dir, lbl2id_map, sequence_len, max_ratio, phase='train', pad=0):

        if phase == 'train':
            self.img_dir = os.path.join(dataset_root_dir, 'train')
            self.lbl_path = os.path.join(dataset_root_dir, 'train_gt.txt')
        else:
            self.img_dir = os.path.join(dataset_root_dir, 'valid')
            self.lbl_path = os.path.join(dataset_root_dir, 'valid_gt.txt')
        self.lbl2id_map = lbl2id_map
        self.pad = pad   # paddingæ ‡è¯†ç¬¦çš„idï¼Œé»˜è®¤0
        self.sequence_len = sequence_len    # åºåˆ—é•¿åº¦
        self.max_ratio = max_ratio * 3      # å°†å®½æ‹‰é•¿3å€

        self.imgs_list = []
        self.lbls_list = []
        with open(self.lbl_path, 'r', encoding='utf-8') as reader:
            for line in reader:
                items = line.rstrip().split(',')
                img_name = items[0]  # img_name = 'word_2.png'
                lbl_str = items[1].strip()[1:-1]  # lbl_str = '[06]'

                self.imgs_list.append(img_name)
                self.lbls_list.append(lbl_str)

        # å®šä¹‰éšæœºé¢œè‰²å˜æ¢
        self.color_trans = transforms.ColorJitter(0.1, 0.1, 0.1)
        # å®šä¹‰ Normalize
        self.trans_Normalize = transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
        ])

    def __getitem__(self, index):
        """ 
        è·å–å¯¹åº”indexçš„å›¾åƒå’Œground truth labelï¼Œå¹¶è§†æƒ…å†µè¿›è¡Œæ•°æ®å¢å¼º
        """
        img_name = self.imgs_list[index]
        img_path = os.path.join(self.img_dir, img_name)
        lbl_str = self.lbls_list[index]

        # ----------------
        # å›¾ç‰‡é¢„å¤„ç†
        # ----------------
        # load image
        img = Image.open(img_path).convert('RGB')

        # å¯¹å›¾ç‰‡è¿›è¡Œå¤§è‡´ç­‰æ¯”ä¾‹çš„ç¼©æ”¾
        # å°†é«˜ç¼©æ”¾åˆ°32ï¼Œå®½å¤§è‡´ç­‰æ¯”ä¾‹ç¼©æ”¾ï¼Œä½†è¦è¢«32æ•´é™¤
        w, h = img.size
        ratio = round((w / h) * 3)   # å°†å®½æ‹‰é•¿3å€ï¼Œç„¶åå››èˆäº”å…¥
        if ratio == 0:
            ratio = 1
        if ratio > self.max_ratio:
            ratio = self.max_ratio
        h_new = 32
        w_new = h_new * ratio
        img_resize = img.resize((w_new, h_new), Image.BILINEAR)

        # å¯¹å›¾ç‰‡å³åŠè¾¹è¿›è¡Œpaddingï¼Œä½¿å¾—å®½/é«˜æ¯”ä¾‹å›ºå®š=self.max_ratio
        img_padd = Image.new('RGB', (32*self.max_ratio, 32), (0, 0, 0))
        img_padd.paste(img_resize, (0, 0))

        # éšæœºé¢œè‰²å˜æ¢
        img_input = self.color_trans(img_padd)
        # Normalize
        img_input = self.trans_Normalize(img_input)

        # ----------------
        # labelå¤„ç†
        # ----------------

        # æ„é€ encoderçš„mask
        encode_mask = [1] * ratio + [0] * (self.max_ratio - ratio)
        encode_mask = torch.tensor(encode_mask)
        encode_mask = (encode_mask != 0).unsqueeze(0)

        # æ„é€ ground truth label
        gt = []
        gt.append(1)    # å…ˆæ·»åŠ å¥å­èµ·å§‹ç¬¦
        for lbl in lbl_str:
            gt.append(self.lbl2id_map[lbl])
        gt.append(2)
        # é™¤å»èµ·å§‹ç¬¦ç»ˆæ­¢ç¬¦ï¼Œlblé•¿åº¦ä¸ºsequence_lenï¼Œå‰©ä¸‹çš„padding
        for i in range(len(lbl_str), self.sequence_len):
            gt.append(0)
        # æˆªæ–­ä¸ºé¢„è®¾çš„æœ€å¤§åºåˆ—é•¿åº¦
        gt = gt[:self.sequence_len]

        # decoderçš„è¾“å…¥
        decode_in = gt[:-1]
        decode_in = torch.tensor(decode_in)
        # decoderçš„è¾“å‡º
        decode_out = gt[1:]
        decode_out = torch.tensor(decode_out)
        # decoderçš„mask
        decode_mask = self.make_std_mask(decode_in, self.pad)
        # æœ‰æ•ˆtokensæ•°
        ntokens = (decode_out != self.pad).data.sum()

        return img_input, encode_mask, decode_in, decode_out, decode_mask, ntokens

    @staticmethod
    def make_std_mask(tgt, pad):
        """
        Create a mask to hide padding and future words.
        padd å’Œ future words å‡åœ¨maskä¸­ç”¨0è¡¨ç¤º
        """
        tgt_mask = (tgt != pad)
        tgt_mask = tgt_mask & subsequent_mask(
            tgt.size(-1)).type_as(tgt_mask.data)
        tgt_mask = tgt_mask.squeeze(0)   # subsequentè¿”å›å€¼çš„shapeæ˜¯(1, N, N)
        return tgt_mask

    def __len__(self):
        return len(self.imgs_list)
```

ä¸Šé¢çš„ä»£ç ä¸­è¿˜è®¾è®¡åˆ°å‡ ä¸ªå’Œlabelå¤„ç†ç›¸å…³çš„ç»†èŠ‚ï¼Œå±äºTransformerè®­ç»ƒç›¸å…³çš„é€»è¾‘ï¼Œä¹‹å‰çš„ç« èŠ‚å·²ç»ä»‹ç»è¿‡ï¼Œè¿™é‡Œå†ç®€å•æä¸€ä¸‹ï¼š

**encode_mask**

ç”±äºæˆ‘ä»¬å¯¹å›¾åƒè¿›è¡Œäº†å°ºå¯¸è°ƒæ•´ï¼Œå¹¶æ ¹æ®éœ€æ±‚å¯¹å›¾åƒè¿›è¡Œäº†paddingï¼Œè€Œpaddingçš„ä½ç½®æ˜¯æ²¡æœ‰åŒ…å«æœ‰æ•ˆä¿¡æ¯çš„ï¼Œä¸ºæ­¤éœ€è¦æ ¹æ®paddingæ¯”ä¾‹æ„é€ ç›¸åº”encode_maskï¼Œè®©transformeråœ¨è®¡ç®—æ—¶å¿½ç•¥è¿™éƒ¨åˆ†æ— æ„ä¹‰çš„åŒºåŸŸã€‚

**labelå¤„ç†**

æœ¬å®éªŒä½¿ç”¨çš„é¢„æµ‹æ ‡ç­¾ä¸æœºå™¨ç¿»è¯‘æ¨¡å‹è®­ç»ƒæ—¶çš„æ ‡ç­¾åŸºæœ¬ä¸€è‡´ï¼Œå› æ­¤åœ¨å¤„ç†æ–¹å¼ä¸­å·®å¼‚è¾ƒå°ã€‚

æ ‡ç­¾å¤„ç†ä¸­ï¼Œå°†labelä¸­å­—ç¬¦è½¬æ¢æˆå…¶å¯¹åº”idï¼Œå¹¶åœ¨å¥å­å¼€å§‹æ·»åŠ èµ·å§‹ç¬¦ï¼Œå¥å­æœ€åæ·»åŠ ç»ˆæ­¢ç¬¦ï¼Œå¹¶åœ¨ä¸æ»¡è¶³sequence_lené•¿åº¦æ—¶åœ¨å‰©ä½™ä½ç½®è¿›è¡Œpaddingï¼ˆ0è¡¥ä½ï¼‰ã€‚

**decode_mask**

ä¸€èˆ¬çš„åœ¨decoderä¸­æˆ‘ä»¬ä¼šæ ¹æ®labelçš„sequence_lenç”Ÿæˆä¸€ä¸ªä¸Šä¸‰è§’é˜µå½¢å¼çš„maskï¼Œmaskçš„æ¯ä¸€è¡Œä¾¿å¯ä»¥æ§åˆ¶å½“å‰time_stepæ—¶ï¼Œåªå…è®¸decoderè·å–å½“å‰æ­¥æ—¶ä¹‹å‰çš„å­—ç¬¦ä¿¡æ¯ï¼Œè€Œç¦æ­¢è·å–æœªæ¥æ—¶åˆ»çš„å­—ç¬¦ä¿¡æ¯ï¼Œè¿™é˜²æ­¢äº†æ¨¡å‹è®­ç»ƒæ—¶çš„ä½œå¼Šè¡Œä¸ºã€‚

decode_maskç»è¿‡ä¸€ä¸ªç‰¹æ®Šçš„å‡½æ•° **make_std_mask()** è¿›è¡Œç”Ÿæˆã€‚

åŒæ—¶ï¼Œdecoderçš„labelåˆ¶ä½œåŒæ ·è¦è€ƒè™‘ä¸Šå¯¹paddingçš„éƒ¨åˆ†è¿›è¡Œmaskï¼Œæ‰€ä»¥decode_maskåœ¨labelè¢«paddingå¯¹åº”çš„ä½ç½®å¤„ä¹Ÿåº”è¯¥è¿›è¡Œå†™æˆFalseã€‚

ç”Ÿæˆçš„decode_maskå¦‚ä¸‹å›¾æ‰€ç¤ºï¼š
![decode_maskç”Ÿæˆ](img/decode_mask.png)

ä»¥ä¸Šæ˜¯æ„å»ºDatasetçš„æ‰€æœ‰ç»†èŠ‚ï¼Œè¿›è€Œæˆ‘ä»¬å¯ä»¥æ„å»ºå‡ºDataLoaderä¾›è®­ç»ƒä½¿ç”¨

### DataLoaderæ„å»º


```python
# æ„é€  dataloader
max_ratio = 8    # å›¾ç‰‡é¢„å¤„ç†æ—¶ å®½/é«˜ çš„æœ€å¤§å€¼ï¼Œä¸è¶…è¿‡å°±ä¿æ¯”ä¾‹resizeï¼Œè¶…è¿‡ä¼šå¼ºè¡Œå‹ç¼©
train_dataset = Recognition_Dataset(
    base_data_dir, lbl2id_map, sequence_len, max_ratio, 'train', pad=0)
valid_dataset = Recognition_Dataset(
    base_data_dir, lbl2id_map, sequence_len, max_ratio, 'valid', pad=0)
# loader size info:
# --> img_input: [batch_size, c, h, w] --> [64, 3, 32, 32*8*3]
# --> encode_mask: [batch_size, h/32, w/32] --> [64, 1, 24] æœ¬æ–‡backboneé‡‡ç”¨çš„32å€ä¸‹é‡‡æ ·ï¼Œæ‰€ä»¥é™¤ä»¥32
# --> decode_in: [batch_size, sequence_len-1] --> [64, 20]
# --> decode_out: [batch_size, sequence_len-1] --> [64, 20]
# --> decode_mask: [batch_size, sequence_len-1, sequence_len-1] --> [64, 20, 20]
# --> ntokens: [batch_size] --> [64]
train_loader = torch.utils.data.DataLoader(train_dataset,
                                           batch_size=batch_size,
                                           shuffle=True,
                                           num_workers=2)
valid_loader = torch.utils.data.DataLoader(valid_dataset,
                                           batch_size=batch_size,
                                           shuffle=False,
                                           num_workers=2)
```

## æ¨¡å‹æ„å»º

ä»£ç é€šè¿‡ **make_ocr_model** å’Œ **OCR_EncoderDecoder** ç±»å®Œæˆæ¨¡å‹ç»“æ„æ­å»ºã€‚

å¯ä»¥ä» **make_ocr_model** è¿™ä¸ªå‡½æ•°çœ‹èµ·ï¼Œè¯¥å‡½æ•°é¦–å…ˆè°ƒç”¨äº†pytorchä¸­é¢„è®­ç»ƒçš„Resnet-18ä½œä¸ºbackboneä»¥æå–å›¾åƒç‰¹å¾ï¼Œæ­¤å¤„ä¹Ÿå¯ä»¥æ ¹æ®è‡ªå·±éœ€è¦è°ƒæ•´ä¸ºå…¶ä»–çš„ç½‘ç»œï¼Œä½†éœ€è¦é‡ç‚¹å…³æ³¨çš„æ˜¯ç½‘ç»œçš„ä¸‹é‡‡æ ·å€æ•°ï¼Œä»¥åŠæœ€åä¸€å±‚ç‰¹å¾å›¾çš„channel_numï¼Œç›¸å…³æ¨¡å—çš„å‚æ•°éœ€è¦åŒæ­¥è°ƒæ•´ã€‚ä¹‹åè°ƒç”¨äº† **OCR_EncoderDecoder** ç±»å®Œæˆtransformerçš„æ­å»ºã€‚æœ€åå¯¹æ¨¡å‹å‚æ•°è¿›è¡Œåˆå§‹åŒ–ã€‚

åœ¨ **OCR_EncoderDecoder** ç±»ä¸­ï¼Œè¯¥ç±»ç›¸å½“äºæ˜¯ä¸€ä¸ªtransformerå„åŸºç¡€ç»„ä»¶çš„æ‹¼è£…çº¿ï¼ŒåŒ…æ‹¬ encoder å’Œ decoder ç­‰ï¼Œå…¶åˆå§‹å‚æ•°æ˜¯å·²å­˜åœ¨çš„åŸºæœ¬ç»„ä»¶ï¼Œå…¶åŸºæœ¬ç»„ä»¶ä»£ç éƒ½åœ¨transformer.pyæ–‡ä»¶ä¸­ï¼Œæœ¬æ–‡å°†ä¸åœ¨è¿‡å¤šå™è¿°ã€‚

è¿™é‡Œå†æ¥å›é¡¾ä¸€ä¸‹ï¼Œå›¾ç‰‡ç»è¿‡backboneåï¼Œå¦‚ä½•æ„é€ ä¸ºTransformerçš„è¾“å…¥ï¼š

å›¾ç‰‡ç»è¿‡backboneåå°†è¾“å‡ºä¸€ä¸ªç»´åº¦ä¸º **[batch_size, 512, 1, 24]** çš„ç‰¹å¾å›¾ï¼Œåœ¨ä¸å…³æ³¨batch_sizeçš„å‰æä¸‹ï¼Œæ¯ä¸€å¼ å›¾åƒéƒ½ä¼šå¾—åˆ°å¦‚ä¸‹æ‰€ç¤ºå…·æœ‰512ä¸ªé€šé“çš„1Ã—24çš„ç‰¹å¾å›¾ï¼Œå¦‚å›¾ä¸­çº¢è‰²æ¡†æ ‡æ³¨æ‰€ç¤ºï¼Œå°†ä¸åŒé€šé“ç›¸åŒä½ç½®çš„ç‰¹å¾å€¼æ‹¼æ¥ç»„æˆä¸€ä¸ªæ–°çš„å‘é‡ï¼Œå¹¶ä½œä¸ºä¸€ä¸ªæ—¶é—´æ­¥çš„è¾“å…¥ï¼Œæ­¤æ—¶å˜æ„é€ å‡ºäº†ç»´åº¦ä¸º **[batch_size, 24, 512]** çš„è¾“å…¥ï¼Œæ»¡è¶³Transformerçš„è¾“å…¥è¦æ±‚ã€‚
![encodeçš„è¿‡ç¨‹](img/encoder.png)

ä¸‹é¢æ¥çœ‹ä¸‹å®Œæ•´çš„æ„é€ æ¨¡å‹éƒ¨åˆ†çš„ä»£ç ï¼š


```python
# æ¨¡å‹ç»“æ„
class OCR_EncoderDecoder(nn.Module):
    """
    A standard Encoder-Decoder architecture.
    Base for this and many other models.
    """

    def __init__(self, encoder, decoder, src_embed, src_position, tgt_embed, generator):
        super(OCR_EncoderDecoder, self).__init__()
        self.encoder = encoder
        self.decoder = decoder
        self.src_embed = src_embed    # input embedding module
        self.src_position = src_position
        self.tgt_embed = tgt_embed    # ouput embedding module
        self.generator = generator    # output generation module

    def forward(self, src, tgt, src_mask, tgt_mask):
        "Take in and process masked src and target sequences."
        # src --> [bs, 3, 32, 768]  [bs, c, h, w]
        # src_mask --> [bs, 1, 24]  [bs, h/32, w/32]
        memory = self.encode(src, src_mask)
        # memory --> [bs, 24, 512]
        # tgt --> decode_in [bs, 20]  [bs, sequence_len-1]
        # tgt_mask --> decode_mask [bs, 20]  [bs, sequence_len-1]
        res = self.decode(memory, src_mask, tgt, tgt_mask)  # [bs, 20, 512]
        return res

    def encode(self, src, src_mask):
        # feature extract
        # src --> [bs, 3, 32, 768]
        src_embedds = self.src_embed(src)
        # æ­¤å¤„ä½¿ç”¨çš„resnet18ä½œä¸ºbackbone è¾“å‡º-->[batchsize, c, h, w] --> [bs, 512, 1, 24]
        # å°†src_embeddsç”±shape(bs, model_dim, 1, max_ratio) å¤„ç†ä¸ºtransformeræœŸæœ›çš„è¾“å…¥shape(bs, æ—¶é—´æ­¥, model_dim)
        # [bs, 512, 1, 24] --> [bs, 24, 512]
        src_embedds = src_embedds.squeeze(-2)
        src_embedds = src_embedds.permute(0, 2, 1)

        # position encode
        src_embedds = self.src_position(src_embedds)  # [bs, 24, 512]

        return self.encoder(src_embedds, src_mask)  # [bs, 24, 512]

    def decode(self, memory, src_mask, tgt, tgt_mask):
        target_embedds = self.tgt_embed(tgt)  # [bs, 20, 512]
        return self.decoder(target_embedds, memory, src_mask, tgt_mask)


def make_ocr_model(tgt_vocab, N=6, d_model=512, d_ff=2048, h=8, dropout=0.1):
    """
    æ„å»ºæ¨¡å‹
    params:
        tgt_vocab: è¾“å‡ºçš„è¯å…¸å¤§å°
        N: ç¼–ç å™¨å’Œè§£ç å™¨å †å åŸºç¡€æ¨¡å—çš„ä¸ªæ•°
        d_model: æ¨¡å‹ä¸­embeddingçš„sizeï¼Œé»˜è®¤512
        d_ff: FeedForward Layerå±‚ä¸­embeddingçš„sizeï¼Œé»˜è®¤2048
        h: MultiHeadAttentionä¸­å¤šå¤´çš„ä¸ªæ•°ï¼Œå¿…é¡»è¢«d_modelæ•´é™¤
        dropout: dropoutçš„æ¯”ç‡
    """
    c = copy.deepcopy

    # torchä¸­é¢„è®­ç»ƒçš„resnet18ä½œä¸ºç‰¹å¾æå–ç½‘ç»œ, backbone
    backbone = models.resnet18(pretrained=True)
    # å»æ‰æœ€åä¸¤ä¸ªå±‚ (global average pooling and fc layer)
    backbone = nn.Sequential(*list(backbone.children())[:-2])

    attn = MultiHeadedAttention(h, d_model)
    ff = PositionwiseFeedForward(d_model, d_ff, dropout)
    position = PositionalEncoding(d_model, dropout)
    # æ„å»ºæ¨¡å‹
    model = OCR_EncoderDecoder(
        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),
        Decoder(DecoderLayer(d_model, c(attn), c(attn), c(ff), dropout), N),
        backbone,
        c(position),
        nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)),
        Generator(d_model, tgt_vocab))  # æ­¤å¤„çš„generatorå¹¶æ²¡æœ‰åœ¨ç±»å†…è°ƒç”¨

    # Initialize parameters with Glorot / fan_avg.
    for child in model.children():
        if child is backbone:
            # å°†backboneçš„æƒé‡è®¾ä¸ºä¸è®¡ç®—æ¢¯åº¦
            for param in child.parameters():
                param.requires_grad = False
            # é¢„è®­ç»ƒå¥½çš„backboneä¸è¿›è¡Œéšæœºåˆå§‹åŒ–ï¼Œå…¶ä½™æ¨¡å—è¿›è¡Œéšæœºåˆå§‹åŒ–
            continue
        for p in child.parameters():
            if p.dim() > 1:
                nn.init.xavier_uniform_(p)
    return model
```

é€šè¿‡ä¸Šè¿°çš„ä¸¤ä¸ªç±»ï¼Œå¯ä»¥æ–¹ä¾¿æ„å»ºtransformeræ¨¡å‹ï¼š


```python
# build model
# use transformer as ocr recognize model
# æ­¤å¤„æ„å»ºçš„ocr_modelä¸å«æœ‰Generator
tgt_vocab = len(lbl2id_map.keys()) 
d_model = 512
ocr_model = make_ocr_model(tgt_vocab, N=5, d_model=d_model, d_ff=2048, h=8, dropout=0.1)
ocr_model.to(device)
```


    OCR_EncoderDecoder(
      (encoder): Encoder(
        (layers): ModuleList(
          (0): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linears): ModuleList(
                (0): Linear(in_features=512, out_features=512, bias=True)
                (1): Linear(in_features=512, out_features=512, bias=True)
                (2): Linear(in_features=512, out_features=512, bias=True)
                (3): Linear(in_features=512, out_features=512, bias=True)
              )
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (1): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linears): ModuleList(
                (0): Linear(in_features=512, out_features=512, bias=True)
                (1): Linear(in_features=512, out_features=512, bias=True)
                (2): Linear(in_features=512, out_features=512, bias=True)
                (3): Linear(in_features=512, out_features=512, bias=True)
              )
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (2): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linears): ModuleList(
                (0): Linear(in_features=512, out_features=512, bias=True)
                (1): Linear(in_features=512, out_features=512, bias=True)
                (2): Linear(in_features=512, out_features=512, bias=True)
                (3): Linear(in_features=512, out_features=512, bias=True)
              )
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (3): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linears): ModuleList(
                (0): Linear(in_features=512, out_features=512, bias=True)
                (1): Linear(in_features=512, out_features=512, bias=True)
                (2): Linear(in_features=512, out_features=512, bias=True)
                (3): Linear(in_features=512, out_features=512, bias=True)
              )
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (4): EncoderLayer(
            (self_attn): MultiHeadedAttention(
              (linears): ModuleList(
                (0): Linear(in_features=512, out_features=512, bias=True)
                (1): Linear(in_features=512, out_features=512, bias=True)
                (2): Linear(in_features=512, out_features=512, bias=True)
                (3): Linear(in_features=512, out_features=512, bias=True)
              )
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
        )
        (norm): LayerNorm()
      )
      (decoder): Decoder(
        (layers): ModuleList(
          (0): DecoderLayer(
            (self_attn): MultiHeadedAttention(
              (linears): ModuleList(
                (0): Linear(in_features=512, out_features=512, bias=True)
                (1): Linear(in_features=512, out_features=512, bias=True)
                (2): Linear(in_features=512, out_features=512, bias=True)
                (3): Linear(in_features=512, out_features=512, bias=True)
              )
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (src_attn): MultiHeadedAttention(
              (linears): ModuleList(
                (0): Linear(in_features=512, out_features=512, bias=True)
                (1): Linear(in_features=512, out_features=512, bias=True)
                (2): Linear(in_features=512, out_features=512, bias=True)
                (3): Linear(in_features=512, out_features=512, bias=True)
              )
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (2): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (1): DecoderLayer(
            (self_attn): MultiHeadedAttention(
              (linears): ModuleList(
                (0): Linear(in_features=512, out_features=512, bias=True)
                (1): Linear(in_features=512, out_features=512, bias=True)
                (2): Linear(in_features=512, out_features=512, bias=True)
                (3): Linear(in_features=512, out_features=512, bias=True)
              )
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (src_attn): MultiHeadedAttention(
              (linears): ModuleList(
                (0): Linear(in_features=512, out_features=512, bias=True)
                (1): Linear(in_features=512, out_features=512, bias=True)
                (2): Linear(in_features=512, out_features=512, bias=True)
                (3): Linear(in_features=512, out_features=512, bias=True)
              )
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (2): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (2): DecoderLayer(
            (self_attn): MultiHeadedAttention(
              (linears): ModuleList(
                (0): Linear(in_features=512, out_features=512, bias=True)
                (1): Linear(in_features=512, out_features=512, bias=True)
                (2): Linear(in_features=512, out_features=512, bias=True)
                (3): Linear(in_features=512, out_features=512, bias=True)
              )
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (src_attn): MultiHeadedAttention(
              (linears): ModuleList(
                (0): Linear(in_features=512, out_features=512, bias=True)
                (1): Linear(in_features=512, out_features=512, bias=True)
                (2): Linear(in_features=512, out_features=512, bias=True)
                (3): Linear(in_features=512, out_features=512, bias=True)
              )
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (2): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (3): DecoderLayer(
            (self_attn): MultiHeadedAttention(
              (linears): ModuleList(
                (0): Linear(in_features=512, out_features=512, bias=True)
                (1): Linear(in_features=512, out_features=512, bias=True)
                (2): Linear(in_features=512, out_features=512, bias=True)
                (3): Linear(in_features=512, out_features=512, bias=True)
              )
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (src_attn): MultiHeadedAttention(
              (linears): ModuleList(
                (0): Linear(in_features=512, out_features=512, bias=True)
                (1): Linear(in_features=512, out_features=512, bias=True)
                (2): Linear(in_features=512, out_features=512, bias=True)
                (3): Linear(in_features=512, out_features=512, bias=True)
              )
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (2): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (4): DecoderLayer(
            (self_attn): MultiHeadedAttention(
              (linears): ModuleList(
                (0): Linear(in_features=512, out_features=512, bias=True)
                (1): Linear(in_features=512, out_features=512, bias=True)
                (2): Linear(in_features=512, out_features=512, bias=True)
                (3): Linear(in_features=512, out_features=512, bias=True)
              )
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (src_attn): MultiHeadedAttention(
              (linears): ModuleList(
                (0): Linear(in_features=512, out_features=512, bias=True)
                (1): Linear(in_features=512, out_features=512, bias=True)
                (2): Linear(in_features=512, out_features=512, bias=True)
                (3): Linear(in_features=512, out_features=512, bias=True)
              )
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (feed_forward): PositionwiseFeedForward(
              (w_1): Linear(in_features=512, out_features=2048, bias=True)
              (w_2): Linear(in_features=2048, out_features=512, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (sublayer): ModuleList(
              (0): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (2): SublayerConnection(
                (norm): LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
        )
        (norm): LayerNorm()
      )
      (src_embed): Sequential(
        (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
        (4): Sequential(
          (0): BasicBlock(
            (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (1): BasicBlock(
            (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (5): Sequential(
          (0): BasicBlock(
            (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (downsample): Sequential(
              (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
              (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): BasicBlock(
            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (6): Sequential(
          (0): BasicBlock(
            (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (downsample): Sequential(
              (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
              (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): BasicBlock(
            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (7): Sequential(
          (0): BasicBlock(
            (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
            (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (downsample): Sequential(
              (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
              (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): BasicBlock(
            (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
      )
      (src_position): PositionalEncoding(
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (tgt_embed): Sequential(
        (0): Embeddings(
          (lut): Embedding(90, 512)
        )
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (generator): Generator(
        (proj): Linear(in_features=512, out_features=90, bias=True)
      )
    )

## æ¨¡å‹è®­ç»ƒ

æ¨¡å‹è®­ç»ƒä¹‹å‰ï¼Œè¿˜éœ€è¦å®šä¹‰æ¨¡å‹è¯„åˆ¤å‡†åˆ™ã€è¿­ä»£ä¼˜åŒ–å™¨ç­‰ã€‚æœ¬å®éªŒåœ¨è®­ç»ƒæ—¶ï¼Œä½¿ç”¨äº†æ ‡ç­¾å¹³æ»‘ï¼ˆlabel smoothingï¼‰ã€ç½‘ç»œè®­ç»ƒçƒ­èº«ï¼ˆwarmupï¼‰ç­‰ç­–ç•¥ï¼Œä»¥ä¸Šç­–ç•¥çš„è°ƒç”¨å‡½æ•°å‡åœ¨`train_utils.py`æ–‡ä»¶ä¸­ï¼Œæ­¤å¤„ä¸æ¶‰åŠä»¥ä¸Šä¸¤ç§æ–¹æ³•çš„åŸç†åŠä»£ç å®ç°ã€‚

label smoothingå¯ä»¥å°†åŸå§‹çš„ç¡¬æ ‡ç­¾è½¬åŒ–ä¸ºè½¯æ ‡ç­¾ï¼Œä»è€Œå¢åŠ æ¨¡å‹çš„å®¹é”™ç‡ï¼Œæå‡æ¨¡å‹æ³›åŒ–èƒ½åŠ›ã€‚ä»£ç ä¸­ **LabelSmoothing()** å‡½æ•°å®ç°äº†label smoothingï¼ŒåŒæ—¶å†…éƒ¨ä½¿ç”¨äº†ç›¸å¯¹ç†µå‡½æ•°è®¡ç®—äº†é¢„æµ‹å€¼ä¸çœŸå®å€¼ä¹‹é—´çš„æŸå¤±ã€‚

warmupç­–ç•¥èƒ½å¤Ÿæœ‰æ•ˆæ§åˆ¶æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­çš„ä¼˜åŒ–å™¨å­¦ä¹ ç‡ï¼Œè‡ªåŠ¨åŒ–çš„å®ç°æ¨¡å‹å­¦ä¹ ç‡ç”±å°å¢å¤§å†é€æ¸ä¸‹é™çš„æ§åˆ¶ï¼Œå¸®åŠ©æ¨¡å‹åœ¨è®­ç»ƒæ—¶æ›´åŠ ç¨³å®šï¼Œå®ç°æŸå¤±çš„å¿«é€Ÿæ”¶æ•›ã€‚ä»£ç ä¸­ **NoamOpt()** å‡½æ•°å®ç°äº†warmupæ§åˆ¶ï¼Œé‡‡ç”¨çš„Adamä¼˜åŒ–å™¨ï¼Œå®ç°å­¦ä¹ ç‡éšè¿­ä»£æ¬¡æ•°çš„è‡ªåŠ¨è°ƒæ•´ã€‚


```python
# train prepare
criterion = LabelSmoothing(size=tgt_vocab, padding_idx=0, smoothing=0.0)  # label smoothing
optimizer = torch.optim.Adam(ocr_model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9)
model_opt = NoamOpt(d_model, 1, 400, optimizer)  # warmup
```

æ¨¡å‹è®­ç»ƒè¿‡ç¨‹çš„ä»£ç å¦‚ä¸‹æ‰€ç¤ºï¼Œæ¯è®­ç»ƒ10ä¸ªepochä¾¿è¿›è¡Œä¸€æ¬¡éªŒè¯ï¼Œå•ä¸ªepochçš„è®¡ç®—è¿‡ç¨‹å°è£…åœ¨ **run_epoch()** å‡½æ•°ä¸­ã€‚

```python
# train & valid ...
for epoch in range(nrof_epochs):
    print(f"\nepoch {epoch}")
	
    print("train...")  # è®­ç»ƒ
    ocr_model.train()
    loss_compute = SimpleLossCompute(ocr_model.generator, criterion, model_opt)
    train_mean_loss = run_epoch(train_loader, ocr_model, loss_compute, device)

    if epoch % 10 == 0:
        print("valid...")  # éªŒè¯
        ocr_model.eval()
        valid_loss_compute = SimpleLossCompute(ocr_model.generator, criterion, None)
        valid_mean_loss = run_epoch(valid_loader, ocr_model, valid_loss_compute, device)
        print(f"valid loss: {valid_mean_loss}")

        # save model
        torch.save(ocr_model.state_dict(), './trained_model/ocr_model.pt')
```

**SimpleLossCompute()** ç±»å®ç°äº†transformerè¾“å‡ºç»“æœçš„lossè®¡ç®—ã€‚åœ¨ä½¿ç”¨è¯¥ç±»ç›´æ¥è®¡ç®—æ—¶ï¼Œç±»éœ€è¦æ¥æ”¶`(x, y, norm)`ä¸‰ä¸ªå‚æ•°ï¼Œ`x`ä¸ºdecoderè¾“å‡ºçš„ç»“æœï¼Œ`y`ä¸ºæ ‡ç­¾æ•°æ®ï¼Œ`norm`ä¸ºlossçš„å½’ä¸€åŒ–ç³»æ•°ï¼Œç”¨batchä¸­æ‰€æœ‰æœ‰æ•ˆtokenæ•°å³å¯ã€‚ç”±æ­¤å¯è§ï¼Œæ­¤å¤„æ‰æ­£å®Œæˆtransformeræ‰€æœ‰ç½‘ç»œçš„æ„å»ºï¼Œå®ç°æ•°æ®è®¡ç®—æµçš„æµé€šã€‚

**run_epoch()** å‡½æ•°å†…éƒ¨å®Œæˆäº†ä¸€ä¸ªepochè®­ç»ƒçš„æ‰€æœ‰å·¥ä½œï¼ŒåŒ…æ‹¬æ•°æ®åŠ è½½ã€æ¨¡å‹æ¨ç†ã€æŸå¤±è®¡ç®—ä¸æ–¹å‘ä¼ æ’­ï¼ŒåŒæ—¶å°†è®­ç»ƒè¿‡ç¨‹ä¿¡æ¯è¿›è¡Œæ‰“å°ã€‚


```python
def run_epoch(data_loader, model, loss_compute, device=None):
    "Standard Training and Logging Function"
    start = time.time()
    total_tokens = 0
    total_loss = 0
    tokens = 0

    for i, batch in enumerate(data_loader):
        img_input, encode_mask, decode_in, decode_out, decode_mask, ntokens = batch
        img_input = img_input.to(device)
        encode_mask = encode_mask.to(device)
        decode_in = decode_in.to(device)
        decode_out = decode_out.to(device)
        decode_mask = decode_mask.to(device)
        ntokens = torch.sum(ntokens).to(device)

        out = model.forward(img_input, decode_in, encode_mask, decode_mask)
        # out --> [bs, 20, 512]  é¢„æµ‹ç»“æœ
        # decode_out --> [bs, 20]  å®é™…ç»“æœ
        # ntokens --> æ ‡ç­¾ä¸­å®é™…æœ‰æ•ˆå­—ç¬¦

        loss = loss_compute(out, decode_out, ntokens)  # æŸå¤±è®¡ç®—
        total_loss += loss
        total_tokens += ntokens
        tokens += ntokens
        if i % 50 == 1:
            elapsed = time.time() - start
            print("Epoch Step: %d Loss: %f Tokens per Sec: %f" %
                  (i, loss / ntokens, tokens / elapsed))
            start = time.time()
            tokens = 0
    return total_loss / total_tokens


class SimpleLossCompute:
    "A simple loss compute and train function."

    def __init__(self, generator, criterion, opt=None):
        self.generator = generator
        self.criterion = criterion
        self.opt = opt

    def __call__(self, x, y, norm):
        """
        norm: lossçš„å½’ä¸€åŒ–ç³»æ•°ï¼Œç”¨batchä¸­æ‰€æœ‰æœ‰æ•ˆtokenæ•°å³å¯
        """
        # x --> out --> [bs, 20, 512]  é¢„æµ‹ç»“æœ
        # y --> decode_out --> [bs, 20]  å®é™…ç»“æœ
        # norm --> ntokens --> æ ‡ç­¾ä¸­å®é™…æœ‰æ•ˆå­—ç¬¦
        x = self.generator(x)
        # label smoothingéœ€è¦å¯¹åº”ç»´åº¦å˜åŒ–
        x_ = x.contiguous().view(-1, x.size(-1))  # [20bs, 512]
        y_ = y.contiguous().view(-1)  # [20bs]
        loss = self.criterion(x_, y_)
        loss /= norm
        loss.backward()
        if self.opt is not None:
            self.opt.step()
            self.opt.optimizer.zero_grad()
        # return loss.data[0] * norm
        return loss.item() * norm
```


```python
# train & valid ...
for epoch in range(nrof_epochs):
    print(f"\nepoch {epoch}")

    print("train...")  # è®­ç»ƒ
    ocr_model.train()
    loss_compute = SimpleLossCompute(ocr_model.generator, criterion, model_opt)
    train_mean_loss = run_epoch(train_loader, ocr_model, loss_compute, device)

    if epoch % 10 == 0:
        print("valid...")  # éªŒè¯
        ocr_model.eval()
        valid_loss_compute = SimpleLossCompute(ocr_model.generator, criterion, None)
        valid_mean_loss = run_epoch(valid_loader, ocr_model, valid_loss_compute, device)
        print(f"valid loss: {valid_mean_loss}")

        # save model
        torch.save(ocr_model.state_dict(), './trained_model/ocr_model.pt')
```


    epoch 0
    train...
    Epoch Step: 1 Loss: 5.285763 Tokens per Sec: 507.413330
    Epoch Step: 51 Loss: 2.954391 Tokens per Sec: 977.676819
    valid...
    Epoch Step: 1 Loss: 3.081431 Tokens per Sec: 632.597046
    valid loss: 2.7926409244537354
    
    epoch 1
    train...
    Epoch Step: 1 Loss: 5.723921 Tokens per Sec: 602.094604
    Epoch Step: 51 Loss: 2.845625 Tokens per Sec: 965.739563
    
    ...
    
    epoch 19
    train...
    Epoch Step: 1 Loss: 2.121170 Tokens per Sec: 553.301636
    Epoch Step: 51 Loss: 2.291446 Tokens per Sec: 973.108887


## é¢„è®­ç»ƒæ¨¡å‹åŠ è½½

### pretrain modelçš„åŠ è½½
```python
ocr_model.load_state_dict(torch.load(model_save_path, map_location=device))
```

## è®­ç»ƒè¿‡ç¨‹çš„å®Œæ•´ä»£ç 


```python
pretrain_model = bool(int(input("Whether to use pretrain_model?(1 or 0)\t")))

if not pretrain_model:
    # train prepare
    criterion = LabelSmoothing(size=tgt_vocab, padding_idx=0, smoothing=0.0)  # label smoothing
    optimizer = torch.optim.Adam(ocr_model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9)
    model_opt = NoamOpt(d_model, 1, 400, optimizer)  # warmup
    
    # train & valid ...
    for epoch in range(nrof_epochs):
        print(f"\nepoch {epoch}")

        print("train...")  # è®­ç»ƒ
        ocr_model.train()
        loss_compute = SimpleLossCompute(ocr_model.generator, criterion, model_opt)
        train_mean_loss = run_epoch(train_loader, ocr_model, loss_compute, device)

        if epoch % 10 == 0:
            print("valid...")  # éªŒè¯
            ocr_model.eval()
            valid_loss_compute = SimpleLossCompute(ocr_model.generator, criterion, None)
            valid_mean_loss = run_epoch(valid_loader, ocr_model, valid_loss_compute, device)
            print(f"valid loss: {valid_mean_loss}")

            # save model
            torch.save(ocr_model.state_dict(), './trained_model/ocr_model.pt')

else:
    ocr_model.load_state_dict(torch.load(model_save_path, map_location=device))
```

    Whether to use pretrain_model?(1 or 0)	1


## è´ªå¿ƒè§£ç 

æ–¹ä¾¿èµ·è§ï¼Œæˆ‘ä»¬ä½¿ç”¨æœ€ç®€å•çš„è´ªå¿ƒè§£ç ç›´æ¥è¿›è¡ŒOCRç»“æœé¢„æµ‹ã€‚å› ä¸ºæ¨¡å‹æ¯ä¸€æ¬¡åªä¼šäº§ç”Ÿä¸€ä¸ªè¾“å‡ºï¼Œæˆ‘ä»¬é€‰æ‹©è¾“å‡ºçš„æ¦‚ç‡åˆ†å¸ƒä¸­çš„æœ€é«˜æ¦‚ç‡å¯¹åº”çš„å­—ç¬¦ä¸ºæœ¬æ¬¡é¢„æµ‹çš„ç»“æœï¼Œç„¶åé¢„æµ‹ä¸‹ä¸€ä¸ªå­—ç¬¦ï¼Œè¿™å°±æ˜¯æ‰€è°“çš„è´ªå¿ƒè§£ç ï¼Œè§ä»£ç ä¸­ **greedy_decode()** å‡½æ•°ã€‚

å®éªŒä¸­åˆ†åˆ«å°†æ¯ä¸€å¼ å›¾åƒä½œä¸ºæ¨¡å‹çš„è¾“å…¥ï¼Œé€å¼ è¿›è¡Œè´ªå¿ƒè§£ç ç»Ÿè®¡æ­£ç¡®ç‡ï¼Œå¹¶æœ€ç»ˆç»™å‡ºäº†è®­ç»ƒé›†å’ŒéªŒè¯é›†å„è‡ªçš„é¢„æµ‹å‡†ç¡®ç‡ã€‚


```python
# greedy decode
def greedy_decode(model, src, src_mask, max_len, start_symbol, end_symbol):
    memory = model.encode(src, src_mask)
    # ysä»£è¡¨ç›®å‰å·²ç”Ÿæˆçš„åºåˆ—ï¼Œæœ€åˆä¸ºä»…åŒ…å«ä¸€ä¸ªèµ·å§‹ç¬¦çš„åºåˆ—ï¼Œä¸æ–­å°†é¢„æµ‹ç»“æœè¿½åŠ åˆ°åºåˆ—æœ€å
    ys = torch.ones(1, 1).fill_(start_symbol).type_as(src.data).long()
    for i in range(max_len-1):
        out = model.decode(memory, src_mask, ys, subsequent_mask(ys.size(1)).type_as(src.data))
        prob = model.generator(out[:, -1])
        _, next_word = torch.max(prob, dim=1)
        next_word = next_word.data[0]
        next_word = torch.ones(1, 1).type_as(src.data).fill_(next_word).long()
        ys = torch.cat([ys, next_word], dim=1)

        next_word = int(next_word)
        if next_word == end_symbol:
            break
        #ys = torch.cat([ys, torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=1)
    ys = ys[0, 1:]
    return ys


def judge_is_correct(pred, label):
    # åˆ¤æ–­æ¨¡å‹é¢„æµ‹ç»“æœå’Œlabelæ˜¯å¦ä¸€è‡´
    pred_len = pred.shape[0]
    label = label[:pred_len]
    is_correct = 1 if label.equal(pred) else 0
    return is_correct
```


```python
# è®­ç»ƒç»“æŸï¼Œä½¿ç”¨è´ªå¿ƒçš„è§£ç æ–¹å¼æ¨ç†è®­ç»ƒé›†å’ŒéªŒè¯é›†ï¼Œç»Ÿè®¡æ­£ç¡®ç‡
ocr_model.eval()

print("\n------------------------------------------------")
print("greedy decode trainset")
total_img_num = 0
total_correct_num = 0
for batch_idx, batch in enumerate(train_loader):
    img_input, encode_mask, decode_in, decode_out, decode_mask, ntokens = batch
    img_input = img_input.to(device)
    encode_mask = encode_mask.to(device)

    # è·å–å•å¼ å›¾åƒä¿¡æ¯
    bs = img_input.shape[0]
    for i in range(bs):
        cur_img_input = img_input[i].unsqueeze(0)
        cur_encode_mask = encode_mask[i].unsqueeze(0)
        cur_decode_out = decode_out[i].cpu()
        # è´ªå¿ƒè§£ç 
        pred_result = greedy_decode(ocr_model, cur_img_input, cur_encode_mask, max_len=sequence_len, start_symbol=1, end_symbol=2)
        pred_result = pred_result.cpu()
        # åˆ¤æ–­é¢„æµ‹æ˜¯å¦æ­£ç¡®
        is_correct = judge_is_correct(pred_result, cur_decode_out)
        total_correct_num += is_correct
        total_img_num += 1
        if not is_correct:
            # é¢„æµ‹é”™è¯¯çš„caseè¿›è¡Œæ‰“å°
            print("----")
            print(cur_decode_out)
            print(pred_result)
total_correct_rate = total_correct_num / total_img_num * 100
print(f"total correct rate of trainset: {total_correct_rate}%")

# ä¸è®­ç»ƒé›†è§£ç ä»£ç ç›¸åŒ
print("\n------------------------------------------------")
print("greedy decode validset")
total_img_num = 0
total_correct_num = 0
for batch_idx, batch in enumerate(valid_loader):
    img_input, encode_mask, decode_in, decode_out, decode_mask, ntokens = batch
    img_input = img_input.to(device)
    encode_mask = encode_mask.to(device)

    bs = img_input.shape[0]
    for i in range(bs):
        cur_img_input = img_input[i].unsqueeze(0)
        cur_encode_mask = encode_mask[i].unsqueeze(0)
        cur_decode_out = decode_out[i].cpu()

        pred_result = greedy_decode(ocr_model, cur_img_input, cur_encode_mask, max_len=sequence_len, start_symbol=1, end_symbol=2)
        pred_result = pred_result.cpu()

        is_correct = judge_is_correct(pred_result, cur_decode_out)
        total_correct_num += is_correct
        total_img_num += 1
        if not is_correct:
            # é¢„æµ‹é”™è¯¯çš„caseè¿›è¡Œæ‰“å°
            print("----")
            pred_len = pred_result.shape[0]
            print(cur_decode_out[:pred_len])
            print(pred_result)
total_correct_rate = total_correct_num / total_img_num * 100
print(f"total correct rate of validset: {total_correct_rate}%")
```


    ------------------------------------------------
    greedy decode trainset
    ----
    tensor([78, 46, 88,  5, 53, 79, 46,  5, 59,  9,  7, 46,  7, 65,  4,  4,  2,  0,
             0,  0])
    tensor([78, 46, 88,  5, 53, 79, 46,  5, 59,  9,  7, 46,  7, 65,  4,  4,  4,  2])
    ----
    tensor([12, 27, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 26, 26, 27, 22,  2,
             0,  0])
    tensor([12, 27, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 26, 26, 26, 27, 22,
             2])
    ----
    tensor([17, 32, 18, 19, 31, 50, 30, 10, 30, 10, 17, 32, 41, 55, 55, 55,  2,  0,
             0,  0])
    tensor([17, 32, 18, 19, 31, 50, 30, 10, 30, 10, 17, 32, 41, 55, 55, 55, 55, 55,
            55, 55])
    ----
    tensor([17, 32, 18, 19, 31, 50, 30, 10, 17, 32, 41, 55, 55,  2,  0,  0,  0,  0,
             0,  0])
    tensor([17, 32, 18, 19, 31, 50, 30, 10, 17, 32, 41, 55, 55, 55,  2])
    ----
    tensor([39, 12, 27, 20, 27, 12, 12, 27, 51,  2,  0,  0,  0,  0,  0,  0,  0,  0,
             0,  0])
    tensor([39, 12, 27, 20, 27, 12, 27, 51,  2])
    ----
    tensor([57, 26, 24, 47, 43, 45, 24, 13, 13, 23, 25, 47,  2,  0,  0,  0,  0,  0,
             0,  0])
    tensor([57, 26, 24, 47, 43, 45, 24, 13, 23, 25, 47,  2])
    total correct rate of trainset: 99.86130374479889%
    
    ------------------------------------------------
    greedy decode validset
    ----
    tensor([20, 12, 24, 35,  2,  0])
    tensor([20, 12, 27, 27, 13,  2])
    ----
    tensor([19, 27, 47, 21, 26, 11, 34, 23])
    tensor([19, 27, 13, 11, 26, 11, 22,  2])
    ----
    ...
    ----
    tensor([33, 11, 13,  2])
    tensor([18, 37, 15,  2])
    ----
    tensor([10, 11, 28, 27, 25, 11, 47, 45,  2])
    tensor([15, 10, 11, 28, 27, 25, 11, 22,  2])
    ----
    tensor([63, 15, 15, 39,  2,  0])
    tensor([49, 17, 48, 63, 64,  2])
    total correct rate of validset: 86.09437751004016%

